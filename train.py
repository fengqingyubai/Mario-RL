import os
import argparse
import multiprocessing
import swanlab
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList
from common.env_utils import get_vec_env
from common.swanlab_callback import SwanLabCallback

# æ¥æ”¶å‘½ä»¤è¡Œå‚æ•° (è¿™æ˜¯åˆ†å…³å¡è®­ç»ƒçš„å…³é”®)
def parse_args():
    parser = argparse.ArgumentParser(description="Train Mario RL per level")
    parser.add_argument("--world", type=int, required=True, help="World ID (1-8)")
    parser.add_argument("--stage", type=int, required=True, help="Stage ID (1-4)")
    parser.add_argument("--gpu_id", type=int, default=0, help="CUDA Device ID")
    parser.add_argument("--num_envs", type=int, default=8, help="Number of parallel environments")
    return parser.parse_args()

if __name__ == '__main__':
    args = parse_args()
    
    WORLD = args.world
    STAGE = args.stage
    
    # è®¾ç½®å¯è§æ˜¾å¡
    os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu_id)

    print(f"ğŸš€ å¯åŠ¨ä¸“ç”¨è®­ç»ƒ: World {WORLD}-{STAGE} | Envs: {args.num_envs}")

    # --- SwanLab åˆå§‹åŒ– ---
    swanlab.init(
        project="SuperMario-RL-AllLevels", 
        experiment_name=f"Level-{WORLD}-{STAGE}", 
        description=f"ä¸“äº«æ¨¡å‹è®­ç»ƒ: {WORLD}-{STAGE}",
        config={
            "algorithm": "PPO",
            "world": WORLD,
            "stage": STAGE,
            "num_envs": args.num_envs,
            # === Viet Nguyen å¤åˆ»ç‰ˆå‚æ•° ===
            "learning_rate": 1e-4,
            "n_steps": 512,
            "batch_size": 256,
            "n_epochs": 10,
            "gamma": 0.9,
            "gae_lambda": 1.0,
            "ent_coef": 0.01,
            "clip_range": 0.2,       
            "max_grad_norm": 0.5,    
            "vf_coef": 0.5,          
            "total_timesteps": 1500000 
        }
    )

    # ç‹¬ç«‹çš„ä¿å­˜ç›®å½•
    CHECKPOINT_DIR = f'./checkpoints/level_{WORLD}_{STAGE}/'
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)

    # --- åˆ›å»ºä¸“ç”¨ç¯å¢ƒ (ä¿®å¤ç‚¹ï¼šä¼ å…¥ world å’Œ stage) ---
    env = get_vec_env(world=WORLD, stage=STAGE, num_envs=args.num_envs)

    # --- æ„å»ºæ¨¡å‹ ---
    model = PPO(
        'CnnPolicy', 
        env, 
        verbose=1, 
        # è¯»å–é…ç½®å‚æ•°
        learning_rate=swanlab.config["learning_rate"], 
        n_steps=swanlab.config["n_steps"],     
        batch_size=swanlab.config["batch_size"], 
        n_epochs=swanlab.config["n_epochs"],
        gamma=swanlab.config["gamma"],
        gae_lambda=swanlab.config["gae_lambda"],
        ent_coef=swanlab.config["ent_coef"],
        clip_range=swanlab.config["clip_range"],
        max_grad_norm=swanlab.config["max_grad_norm"],
        vf_coef=swanlab.config["vf_coef"],
        
        device="cuda", 
        tensorboard_log=None 
    )

    # --- è®­ç»ƒå‚æ•° ---
    TOTAL_TIMESTEPS = swanlab.config["total_timesteps"]
    
    # æ¯ 50ä¸‡æ­¥ä¿å­˜ä¸€æ¬¡
    save_freq = max(1, 500000 // args.num_envs)

    callbacks = CallbackList([
        CheckpointCallback(save_freq=save_freq, save_path=CHECKPOINT_DIR, name_prefix=f'mario_{WORLD}_{STAGE}'),
        SwanLabCallback()
    ])

    try:
        model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=callbacks) 
    except KeyboardInterrupt:
        print(f"Level {WORLD}-{STAGE} ä¸­æ–­ï¼Œä¿å­˜ä¸­...")
    finally:
        model.save(f"final_model_level_{WORLD}_{STAGE}")
        swanlab.finish()
        env.close()
        print(f"Level {WORLD}-{STAGE} è®­ç»ƒç»“æŸã€‚")