import os
import multiprocessing
import swanlab
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList
from common.env_utils import get_vec_env
from common.swanlab_callback import SwanLabCallback

# --- çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦å™¨ (å¤‡ç”¨ï¼Œå½“å‰é…ç½®ä½¿ç”¨å›ºå®š LR) ---
def linear_schedule(initial_value: float):
    def func(progress_remaining: float) -> float:
        return progress_remaining * initial_value
    return func

if __name__ == '__main__':
    # --- 1. é…ç½® ---
    # æ ¹æ® CPU æ ¸å¿ƒæ•°è‡ªåŠ¨è°ƒæ•´è¿›ç¨‹æ•°ï¼Œæˆ–è€…æ‰‹åŠ¨æŒ‡å®š (å¦‚ 8)
    cpu_count = multiprocessing.cpu_count()
    NUM_ENVS = min(16, max(4, int(cpu_count * 0.8))) 
    
    print(f"ğŸš€ å¯åŠ¨å…¨å…³å¡é€šç”¨è®­ç»ƒ (SuperMarioBros-v0), è¿›ç¨‹æ•°: {NUM_ENVS}")

    # --- 2. åˆå§‹åŒ– SwanLab ---
    swanlab.init(
        project="SuperMario-RL", 
        experiment_name="PPO-Mario-AllLevels-20M",
        description="å…¨å…³å¡è®­ç»ƒ PPO (LR=1e-4, Gamma=0.9, GAE=1.0) - ç›®æ ‡ 2000ä¸‡æ­¥",
        config={
            "algorithm": "PPO",
            "env": "SuperMarioBros-v0", # å…¨å…³å¡
            "num_envs": NUM_ENVS,
            # === æ²¿ç”¨ä¹‹å‰çš„æˆåŠŸå‚æ•° ===
            "learning_rate": 1e-4,      # å›ºå®š 1e-4ï¼Œç¨³å¥
            "n_steps": 512,             # çŸ­é‡‡æ ·
            "batch_size": 256,          # 4096 / 16
            "n_epochs": 10,             
            "gamma": 0.9,               # çŸ­è§†ç­–ç•¥ï¼Œé€‚åˆåŠ¨ä½œæ¸¸æˆ
            "gae_lambda": 1.0,          
            "clip_range": 0.2,
            "ent_coef": 0.01,           
            "max_grad_norm": 0.5,
            "vf_coef": 0.5,
        }
    )

    # æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
    CHECKPOINT_DIR = './checkpoints_general/'
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)

    # --- 3. åˆ›å»ºç¯å¢ƒ ---
    # ä¸å†ä¼ å…¥ world/stageï¼Œé»˜è®¤åŠ è½½å…¨å…³å¡ç¯å¢ƒ
    env = get_vec_env(num_envs=NUM_ENVS)

    # --- 4. æ„å»ºæ¨¡å‹ ---
    model = PPO(
        'CnnPolicy', 
        env, 
        verbose=1, 
        
        # å‚æ•°æ˜ å°„
        learning_rate=swanlab.config["learning_rate"], 
        n_steps=swanlab.config["n_steps"],     
        batch_size=swanlab.config["batch_size"], 
        n_epochs=swanlab.config["n_epochs"],
        gamma=swanlab.config["gamma"],
        gae_lambda=swanlab.config["gae_lambda"],
        clip_range=swanlab.config["clip_range"],
        ent_coef=swanlab.config["ent_coef"],
        max_grad_norm=swanlab.config["max_grad_norm"],
        
        device="cuda", 
        tensorboard_log=None 
    )

    # --- 5. å¼€å§‹è®­ç»ƒ ---
    # ç›®æ ‡ï¼š2000ä¸‡æ­¥ (20M)
    TOTAL_TIMESTEPS = 20000000 
    
    # ä¿å­˜é¢‘ç‡ï¼šæ¯ 50ä¸‡æ­¥ä¿å­˜ä¸€æ¬¡
    # è®¡ç®—æ–¹å¼ï¼š500,000 / è¿›ç¨‹æ•°
    save_freq = max(1, 500000 // NUM_ENVS)

    callbacks = CallbackList([
        CheckpointCallback(save_freq=save_freq, save_path=CHECKPOINT_DIR, name_prefix='mario_general'),
        SwanLabCallback()
    ])

    try:
        print(f"å¼€å§‹è®­ç»ƒ! ç›®æ ‡æ­¥æ•°: {TOTAL_TIMESTEPS}")
        model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=callbacks) 
    except KeyboardInterrupt:
        print("æ£€æµ‹åˆ°ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜æ¨¡å‹...")
    finally:
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        model.save("mario_general_final_20M")
        swanlab.finish()
        env.close()
        print("è®­ç»ƒç»“æŸï¼Œèµ„æºå·²é‡Šæ”¾ã€‚")