import os
import multiprocessing
import swanlab
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList
from common.env_utils import get_vec_env
from common.swanlab_callback import SwanLabCallback

if __name__ == '__main__':
    # --- 1. è®¾ç½®è¦è®­ç»ƒçš„å…³å¡ ---
    # åŸä»£ç æ˜¯é’ˆå¯¹å•å…³å¡è®¾è®¡çš„ï¼Œè¿™é‡Œæˆ‘ä»¬ç»ƒ 1-1
    WORLD = 1
    STAGE = 1
    
    # è¿›ç¨‹æ•° (åŸä»£ç é»˜è®¤ 8)
    # å¦‚æœä½  CPU å¼ºï¼Œå¯ä»¥è®¾ä¸º 8ï¼›å¦‚æœå¼±ï¼ŒSB3 å»ºè®®è®¾ä¸ºæ ¸å¿ƒæ•°
    NUM_ENVS = 8 
    
    print(f"ğŸš€ å¯åŠ¨å¤åˆ»ç‰ˆè®­ç»ƒ: World {WORLD}-{STAGE}, è¿›ç¨‹æ•°: {NUM_ENVS}")

    # --- 2. åˆå§‹åŒ– SwanLab ---
    swanlab.init(
        project="SuperMario-RL", 
        experiment_name=f"PPO-VietNguyen-Rep-1-1",
        description="1:1å¤åˆ»VietNguyenå‚æ•°: LR=1e-4, Gamma=0.9, GAE=1.0, Scoreå¥–åŠ±",
        config={
            "algorithm": "PPO",
            "world": WORLD,
            "stage": STAGE,
            "num_envs": NUM_ENVS,
            # === æ ¸å¿ƒå¤åˆ»å‚æ•° ===
            "learning_rate": 1e-4,      # æ’å®šï¼Œä¸è¡°å‡
            "n_steps": 512,             # æçŸ­çš„é‡‡æ ·é•¿åº¦ (æ›´æ–°é¢‘ç¹)
            "batch_size": 256,          # 4096 / 16 = 256
            "n_epochs": 10,             # æ•°æ®å¤ä¹  10 é
            "gamma": 0.9,               # æåº¦çŸ­è§†ï¼Œåªçœ‹çœ¼å‰
            "gae_lambda": 1.0,          # æ¯”è¾ƒç½•è§çš„è®¾ç½®
            "clip_range": 0.2,
            "ent_coef": 0.01,           # æ ‡å‡†æ¢ç´¢
            "max_grad_norm": 0.5,
            "vf_coef": 0.5,
        }
    )

    CHECKPOINT_DIR = f'./checkpoints_{WORLD}_{STAGE}/'
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)

    # --- 3. åˆ›å»ºç¯å¢ƒ ---
    env = get_vec_env(world=WORLD, stage=STAGE, num_envs=NUM_ENVS)

    # --- 4. æ„å»ºæ¨¡å‹ ---
    model = PPO(
        'CnnPolicy', 
        env, 
        verbose=1, 
        
        # å‚æ•°æ˜ å°„
        learning_rate=swanlab.config["learning_rate"], 
        n_steps=swanlab.config["n_steps"],     
        batch_size=swanlab.config["batch_size"], 
        n_epochs=swanlab.config["n_epochs"],
        gamma=swanlab.config["gamma"],
        gae_lambda=swanlab.config["gae_lambda"],
        clip_range=swanlab.config["clip_range"],
        ent_coef=swanlab.config["ent_coef"],
        max_grad_norm=swanlab.config["max_grad_norm"],
        
        device="cuda", 
        tensorboard_log=None 
    )

    # --- 5. å¼€å§‹è®­ç»ƒ ---
    # åŸä»£ç ç›®æ ‡ 500ä¸‡æ­¥ï¼Œè¿™é‡Œæˆ‘ä»¬è®¾å¤§ä¸€ç‚¹ï¼Œæ‰‹åŠ¨åœæ­¢å³å¯
    TOTAL_TIMESTEPS = 5000000 
    
    # è°ƒæ•´ä¿å­˜é¢‘ç‡ï¼šåŸä»£ç æ¯ 50 æ¬¡ update ä¿å­˜ä¸€æ¬¡
    # 50 updates * 512 steps * 8 envs = 204,800 steps
    # æˆ‘ä»¬è¿™é‡Œç®€åŒ–ä¸ºæ¯ 20ä¸‡æ­¥ä¿å­˜
    save_freq = 200000 // NUM_ENVS

    callbacks = CallbackList([
        CheckpointCallback(save_freq=save_freq, save_path=CHECKPOINT_DIR, name_prefix='mario_viet'),
        SwanLabCallback()
    ])

    try:
        model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=callbacks) 
    except KeyboardInterrupt:
        print("æ£€æµ‹åˆ°ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜æ¨¡å‹...")
    finally:
        model.save(f"mario_viet_final_{WORLD}_{STAGE}")
        swanlab.finish()
        env.close()
        print("è®­ç»ƒç»“æŸã€‚")